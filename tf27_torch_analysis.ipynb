{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f0dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import redis\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from multiprocessing import Process\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f9e39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:44:01.840776: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:44:01.840853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:44:01.840866: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:44:01.841099: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:44:01.841168: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:44:01.841182: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:44:01.842519: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('alexnet-cifar10_origin.h5')\n",
    "new_output = tf.keras.layers.Softmax()(tf.keras.layers.Dense(10)(model.layers[-2].output)) # without specifying activation \n",
    "new_model = tf.keras.models.Model(inputs=model.input, outputs=new_output)\n",
    "new_model.layers[-2].set_weights(model.layers[-1].get_weights())\n",
    "model = new_model\n",
    "\n",
    "# model = tf.keras.applications.ResNet50()\n",
    "# new_output = tf.keras.layers.Softmax()(tf.keras.layers.Dense(1000)(model.layers[-2].output)) # without specifying activation \n",
    "# new_model = tf.keras.models.Model(inputs=model.input, outputs=new_output)\n",
    "# new_model.layers[-2].set_weights(model.layers[-1].get_weights())\n",
    "# model = new_model\n",
    "\n",
    "# model = tf.keras.applications.MobileNetV2()\n",
    "# new_output = tf.keras.layers.Softmax()(tf.keras.layers.Dense(1000)(model.layers[-2].output)) # without specifying activation \n",
    "# new_model = tf.keras.models.Model(inputs=model.input, outputs=new_output)\n",
    "# new_model.layers[-2].set_weights(model.layers[-1].get_weights())\n",
    "# model = new_model\n",
    "\n",
    "# model = tf.keras.applications.VGG16()\n",
    "# new_output = tf.keras.layers.Softmax()(tf.keras.layers.Dense(1000)(model.layers[-2].output)) # without specifying activation \n",
    "# new_model = tf.keras.models.Model(inputs=model.input, outputs=new_output)\n",
    "# new_model.layers[-2].set_weights(model.layers[-1].get_weights())\n",
    "# model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0581ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to tf2.7\n",
    "def import_model(model_name, model, db_flag):\n",
    "    r = redis.Redis(db=db_flag)\n",
    "    w = pickle.loads(r.hget(model_name, 'weights'))\n",
    "    x = pickle.loads(r.hget(model_name, 'input'))\n",
    "\n",
    "    model.set_weights(w)\n",
    "        \n",
    "    return model, x\n",
    "\n",
    "def tf27_outputs(model, x):\n",
    "    # collect model intermediate layers output function\n",
    "    layers_output = []\n",
    "    for l in model.layers:\n",
    "        layers_output.append(l.get_output_at(-1))\n",
    "        \n",
    "    extractor = tf.keras.Model(inputs=model.inputs, outputs=layers_output)\n",
    "    o_tf = extractor.predict(x)\n",
    "    \n",
    "    return o_tf[1:]\n",
    "\n",
    "def tf2torch_outputs(model, x, db_flag):\n",
    "    TORCH_MODEL_DIR = 'torch_model'\n",
    "    if not os.path.exists(TORCH_MODEL_DIR):\n",
    "        os.mkdir(TORCH_MODEL_DIR)\n",
    "        \n",
    "    model_key = 0\n",
    "    input_key = 0\n",
    "    r = redis.Redis(db=db_flag)\n",
    "    r.hset('model', model_key, pickle.dumps(model))\n",
    "    r.hset('input', input_key, pickle.dumps(x))\n",
    "        \n",
    "    # get torch outputs for each model layer\n",
    "    P = []\n",
    "    for i in range(1, len(model.layers)):\n",
    "        cmd = f'/data/yylaiai/anaconda3/envs/fyp_v3/bin/python get_prediction_torch.py {db_flag} {model_key} {input_key} {i}'\n",
    "        p = Process(target=lambda: os.system(cmd))\n",
    "        p.start()\n",
    "        P.append(p)\n",
    "        \n",
    "    for p in P:\n",
    "        p.join()\n",
    "    P = []\n",
    "        \n",
    "    # check if all layer outputs are computed\n",
    "    for i in range(1, len(model.layers)):\n",
    "        if r.hget(f'predictions_{i}', 'torch') == None:\n",
    "            raise Exception(f'Layer {i} not computed')\n",
    "\n",
    "    o_torch = []\n",
    "    for i in range(1, len(model.layers)):\n",
    "        o_torch.append(pickle.loads(r.hget(f'predictions_{i}', 'torch')))\n",
    "        \n",
    "    return o_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af674119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, x = import_model('alexnet', model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dae212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o_tf = tf27_outputs(model, x[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f93828c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ram://968f2673-3ecf-4149-ac52-7762ccd8f1d9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2022-03-27 23:47:39.878784: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.878943: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.878966: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.879373: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.879480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.879500: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.880291: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:39.915897: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.915989: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.916021: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.916344: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.916422: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.916437: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.916960: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.917043: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.917057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.917243: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:39.917285: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.917352: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.917371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.917898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:39.918473: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.918545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.918558: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.918856: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.918934: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.918948: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.919512: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:39.928317: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.928448: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.928472: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.928746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.928816: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.928831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.929332: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:39.932345: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:39.932438: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:39.932458: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:39.932820: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.932927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:39.932949: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:39.933640: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:40.357217: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.357314: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.357329: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.357642: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.357712: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.357727: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.358428: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.363614: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.363714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.363728: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.364131: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.364210: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.364224: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.364821: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.423118: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.423218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.423237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.423505: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.423602: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.423639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.424265: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.432939: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.433065: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.433086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.433376: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.433491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.433514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.434117: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.732983: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.733088: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.733102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.733354: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.733425: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.733440: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.733970: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.767688: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.767791: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.767804: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.768126: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.768198: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.768212: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.768787: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.782539: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.782674: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.782698: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.782981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.783084: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.783103: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.783724: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.784933: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.785026: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.785048: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.785330: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.785441: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.785463: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.786052: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.791667: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.791771: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.791792: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.792005: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.792077: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.792092: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.792555: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.800116: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.800251: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.800273: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.800552: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.800696: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.800727: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.801394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.801753: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.801849: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.801864: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.802467: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.802549: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.802563: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.803220: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 23:47:40.804298: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-27 23:47:40.804448: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ugcpu4\n",
      "2022-03-27 23:47:40.804460: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ugcpu4\n",
      "2022-03-27 23:47:40.804764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.804938: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-03-27 23:47:40.804953: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-03-27 23:47:40.805915: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:42.261019: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:42.329371: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2022-03-27 23:47:42.595062: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:42.697771: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2022-03-27 23:47:42.779753: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:42.823400: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:42.949177: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:43.088667: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:43.471682: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:43.572824: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:43.716067: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:43.905345: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-03-27 23:47:44.138854: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:44.171719: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:44.204338: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:44.448332: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-27 23:47:44.479113: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:45.486299: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:50,689 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:50,694 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:50,694 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:50,697 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:50,796 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:50,869 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:50,870 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:50,870 - INFO - Output names: ['conv2d_1']\n",
      "2022-03-27 23:47:50,917 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:50,917 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:50,917 - INFO - Output names: ['max_pooling2d_1']\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,025 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,027 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,061 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:51,106 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:51,106 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:51,114 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:51,132 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:51,148 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:51,148 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:51,149 - INFO - Output names: ['batch_normalization_1']\n",
      "2022-03-27 23:47:51,153 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:51,154 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:51,154 - INFO - Output names: ['conv2d_2']\n",
      "2022-03-27 23:47:51,182 - INFO - After optimization: Identity -5 (5->0), Transpose -2 (4->2)\n",
      "2022-03-27 23:47:51,190 - INFO - \n",
      "2022-03-27 23:47:51,190 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:51,190 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_2 to ONNX\n",
      "2022-03-27 23:47:51,190 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:51,190 - INFO - Model outputs: ['max_pooling2d_1']\n",
      "2022-03-27 23:47:51,190 - INFO - ONNX model is saved at ./torch_model/tf_model_2.onnx\n",
      "2022-03-27 23:47:51,190 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:51,200 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:51,240 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:51,282 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:51,282 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:51,283 - INFO - Output names: ['conv2d_3']\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,290 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,332 - INFO - After optimization: Identity -5 (5->0)\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:51,347 - INFO - \n",
      "2022-03-27 23:47:51,347 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_1 to ONNX\n",
      "2022-03-27 23:47:51,347 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:51,347 - INFO - Model outputs: ['conv2d_1']\n",
      "2022-03-27 23:47:51,347 - INFO - ONNX model is saved at ./torch_model/tf_model_1.onnx\n",
      "2022-03-27 23:47:51,361 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:51,362 - INFO - Using opset <onnx, 9>\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,416 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,422 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:51,451 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:51,519 - INFO - After optimization: Identity -5 (5->0), Transpose -4 (6->2)\n",
      "2022-03-27 23:47:51,541 - INFO - \n",
      "2022-03-27 23:47:51,541 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_3 to ONNX\n",
      "2022-03-27 23:47:51,542 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:51,542 - INFO - Model outputs: ['batch_normalization_1']\n",
      "2022-03-27 23:47:51,542 - INFO - ONNX model is saved at ./torch_model/tf_model_3.onnx\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,586 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:51,601 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:51,610 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:51,610 - INFO - Using opset <onnx, 9>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:51,729 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:51,758 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:51,758 - INFO - Using opset <onnx, 9>\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:51,908 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:52,064 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:52,103 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:52,148 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:52,148 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:52,149 - INFO - Output names: ['dropout_1']\n",
      "2022-03-27 23:47:52,173 - INFO - After optimization: Identity -5 (5->0), Transpose -6 (8->2)\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:52,196 - INFO - \n",
      "2022-03-27 23:47:52,196 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_4 to ONNX\n",
      "2022-03-27 23:47:52,196 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:52,196 - INFO - Model outputs: ['conv2d_2']\n",
      "2022-03-27 23:47:52,197 - INFO - ONNX model is saved at ./torch_model/tf_model_4.onnx\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "2022-03-27 23:47:52,244 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:52,244 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:52,245 - INFO - Output names: ['conv2d_4']\n",
      "2022-03-27 23:47:52,290 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:52,300 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:52,337 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:52,344 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:52,345 - INFO - Output names: ['max_pooling2d_3']\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:52,443 - INFO - Computed 0 values for constant folding\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:52,527 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:52,529 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    max_pooling2d_1 = StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0.permute(*[0, 2, 3, 1])\n",
      "    return max_pooling2d_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n",
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    conv2d_1 = StatefulPartitionedCall_model_conv2d_1_Relu_0.permute(*[0, 2, 3, 1])\n",
      "    return conv2d_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:52,726 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:52,831 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:52,860 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:52,866 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:52,866 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:52,866 - INFO - Output names: ['conv2d_5']\n",
      "2022-03-27 23:47:52,936 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:52,936 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:52,936 - INFO - Output names: ['max_pooling2d_2']\n",
      "2022-03-27 23:47:52,979 - INFO - After optimization: Identity -5 (5->0), Transpose -12 (14->2)\n",
      "2022-03-27 23:47:53,009 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:53,010 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:53,010 - INFO - Output names: ['batch_normalization_2']\n",
      "2022-03-27 23:47:53,050 - INFO - \n",
      "2022-03-27 23:47:53,051 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_7 to ONNX\n",
      "2022-03-27 23:47:53,051 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:53,051 - INFO - Model outputs: ['conv2d_3']\n",
      "2022-03-27 23:47:53,051 - INFO - ONNX model is saved at ./torch_model/tf_model_7.onnx\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,100 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,118 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:53,119 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:53,120 - INFO - Output names: ['dense_1']\n",
      "2022-03-27 23:47:53,126 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:53,126 - INFO - Using opset <onnx, 9>\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,239 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,267 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,405 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:53,406 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:53,406 - INFO - Output names: ['batch_normalization_3']\n",
      "2022-03-27 23:47:53,465 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:53,466 - INFO - Using opset <onnx, 9>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    batch_normalization_1 = StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0.permute(*[0, 2, 3, 1])\n",
      "    return batch_normalization_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,615 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,632 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:53,672 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:53,673 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:53,719 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:53,719 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:53,850 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:53,988 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:53,988 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:53,989 - INFO - Output names: ['flatten_1']\n",
      "2022-03-27 23:47:53,999 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:54,026 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,026 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,026 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:54,028 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:54,066 - INFO - After optimization: Identity -5 (5->0), Transpose -8 (10->2)\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,086 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,092 - INFO - \n",
      "2022-03-27 23:47:54,093 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_5 to ONNX\n",
      "2022-03-27 23:47:54,093 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:54,093 - INFO - Model outputs: ['max_pooling2d_2']\n",
      "2022-03-27 23:47:54,093 - INFO - ONNX model is saved at ./torch_model/tf_model_5.onnx\n",
      "2022-03-27 23:47:54,094 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:54,247 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:54,248 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:54,255 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:54,364 - INFO - After optimization: Identity -5 (5->0), Transpose -10 (12->2)\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:54,412 - INFO - \n",
      "2022-03-27 23:47:54,412 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_6 to ONNX\n",
      "2022-03-27 23:47:54,412 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:54,412 - INFO - Model outputs: ['batch_normalization_2']\n",
      "2022-03-27 23:47:54,412 - INFO - ONNX model is saved at ./torch_model/tf_model_6.onnx\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:54,540 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    conv2d_2 = StatefulPartitionedCall_model_conv2d_2_Relu_0.permute(*[0, 2, 3, 1])\n",
      "    return conv2d_2\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:54,604 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:54,605 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:47:54,652 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:54,653 - INFO - Using opset <onnx, 9>\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,839 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:54,849 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:54,960 - INFO - After optimization: Identity -5 (5->0), Transpose -14 (16->2)\n",
      "2022-03-27 23:47:55,067 - INFO - \n",
      "2022-03-27 23:47:55,067 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_8 to ONNX\n",
      "2022-03-27 23:47:55,067 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:55,067 - INFO - Model outputs: ['conv2d_4']\n",
      "2022-03-27 23:47:55,067 - INFO - ONNX model is saved at ./torch_model/tf_model_8.onnx\n",
      "2022-03-27 23:47:55,127 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:55,127 - INFO - Using opset <onnx, 9>\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:55,270 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:55,384 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:55,548 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:55,548 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:55,549 - INFO - Output names: ['dropout_2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    conv2d_3 = StatefulPartitionedCall_model_conv2d_3_Relu_0.permute(*[0, 2, 3, 1])\n",
      "    return conv2d_3\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:55,660 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:55,712 - INFO - Computed 0 values for constant folding\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:55,776 - INFO - Computed 0 values for constant folding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    max_pooling2d_2 = StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0.permute(*[0, 2, 3, 1])\n",
      "    return max_pooling2d_2\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:56,029 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:56,129 - INFO - Optimizing ONNX model\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:56,352 - INFO - Computed 0 values for constant folding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    batch_normalization_2 = StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0.permute(*[0, 2, 3, 1])\n",
      "    return batch_normalization_2\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:56,473 - INFO - After optimization: Const +1 (18->19), Identity -5 (5->0), Reshape +1 (0->1), Transpose -19 (20->1)\n",
      "2022-03-27 23:47:56,493 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:47:56,499 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:56,568 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:56,583 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:56,604 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:56,605 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:56,606 - INFO - Output names: ['dense_2']\n",
      "2022-03-27 23:47:56,615 - INFO - \n",
      "2022-03-27 23:47:56,615 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_10 to ONNX\n",
      "2022-03-27 23:47:56,615 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:56,616 - INFO - Model outputs: ['max_pooling2d_3']\n",
      "2022-03-27 23:47:56,616 - INFO - ONNX model is saved at ./torch_model/tf_model_10.onnx\n",
      "2022-03-27 23:47:56,643 - INFO - After optimization: Identity -5 (5->0), Transpose -16 (18->2)\n",
      "/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-03-27 23:47:56,742 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-03-27 23:47:56,773 - INFO - \n",
      "2022-03-27 23:47:56,773 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_9 to ONNX\n",
      "2022-03-27 23:47:56,773 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:56,773 - INFO - Model outputs: ['conv2d_5']\n",
      "2022-03-27 23:47:56,773 - INFO - ONNX model is saved at ./torch_model/tf_model_9.onnx\n",
      "2022-03-27 23:47:57,051 - INFO - After optimization: Const +1 (22->23), Identity -5 (5->0), Reshape +1 (0->1), Transpose -21 (22->1)\n",
      "2022-03-27 23:47:57,137 - INFO - After optimization: Cast -1 (1->0), Identity -6 (6->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:47:57,203 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:57,224 - INFO - \n",
      "2022-03-27 23:47:57,224 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_11 to ONNX\n",
      "2022-03-27 23:47:57,224 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:57,225 - INFO - Model outputs: ['batch_normalization_3']\n",
      "2022-03-27 23:47:57,225 - INFO - ONNX model is saved at ./torch_model/tf_model_11.onnx\n",
      "2022-03-27 23:47:57,276 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:57,277 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:57,278 - INFO - Output names: ['dense']\n",
      "2022-03-27 23:47:57,322 - INFO - \n",
      "2022-03-27 23:47:57,322 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_14 to ONNX\n",
      "2022-03-27 23:47:57,322 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:57,323 - INFO - Model outputs: ['dropout_1']\n",
      "2022-03-27 23:47:57,323 - INFO - ONNX model is saved at ./torch_model/tf_model_14.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    conv2d_4 = StatefulPartitionedCall_model_conv2d_4_Relu_0.permute(*[0, 2, 3, 1])\n",
      "    return conv2d_4\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:57,542 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:47:57,630 - INFO - After optimization: Cast -1 (1->0), Identity -5 (5->0), Transpose -21 (22->1)\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:57,646 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:57,783 - INFO - \n",
      "2022-03-27 23:47:57,784 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_12 to ONNX\n",
      "2022-03-27 23:47:57,784 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:57,784 - INFO - Model outputs: ['flatten_1']\n",
      "2022-03-27 23:47:57,784 - INFO - ONNX model is saved at ./torch_model/tf_model_12.onnx\n",
      "2022-03-27 23:47:57,825 - INFO - Signatures found in model: [serving_default].\n",
      "2022-03-27 23:47:57,826 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-03-27 23:47:57,826 - INFO - Output names: ['softmax']\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    max_pooling2d_3 = torch.reshape(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0, [s if s != 0 else StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0.shape[i] for i, s in enumerate(self._vars[\"new_shape__46\"])])\n",
      "    return max_pooling2d_3\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "2022-03-27 23:47:58,194 - INFO - After optimization: Cast -1 (1->0), Identity -5 (5->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:47:58,367 - INFO - \n",
      "2022-03-27 23:47:58,367 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_13 to ONNX\n",
      "2022-03-27 23:47:58,367 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:47:58,367 - INFO - Model outputs: ['dense_1']\n",
      "2022-03-27 23:47:58,367 - INFO - ONNX model is saved at ./torch_model/tf_model_13.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    conv2d_5 = StatefulPartitionedCall_model_conv2d_5_Relu_0.permute(*[0, 2, 3, 1])\n",
      "    return conv2d_5\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    batch_normalization_3 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"new_shape__50\"])])\n",
      "    return batch_normalization_3\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    dropout_1 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    return dropout_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    flatten_1 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    return flatten_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:59,001 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    dense_1 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    return dense_1\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:47:59,504 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:47:59,505 - INFO - Using opset <onnx, 9>\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:59,739 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "WARNING:tensorflow:From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-03-27 23:47:59,969 - WARNING - From /data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_11/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_14/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_12/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_13/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:48:01,454 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:48:01,454 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:48:01,572 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:48:01,572 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:48:01,842 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.10.0/5cd3b5\n",
      "2022-03-27 23:48:01,842 - INFO - Using opset <onnx, 9>\n",
      "2022-03-27 23:48:05,975 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:48:07,163 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:48:07,477 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:48:07,656 - INFO - Computed 0 values for constant folding\n",
      "2022-03-27 23:48:09,680 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:48:09,958 - INFO - After optimization: Cast -1 (1->0), Identity -7 (7->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:48:10,622 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:48:10,744 - INFO - \n",
      "2022-03-27 23:48:10,744 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_16 to ONNX\n",
      "2022-03-27 23:48:10,744 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:48:10,744 - INFO - Model outputs: ['dropout_2']\n",
      "2022-03-27 23:48:10,744 - INFO - ONNX model is saved at ./torch_model/tf_model_16.onnx\n",
      "2022-03-27 23:48:10,910 - INFO - After optimization: Cast -1 (1->0), Identity -7 (7->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:48:10,959 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:48:11,265 - INFO - After optimization: Cast -1 (1->0), Identity -7 (7->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:48:11,673 - INFO - Optimizing ONNX model\n",
      "2022-03-27 23:48:11,676 - INFO - \n",
      "2022-03-27 23:48:11,676 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_17 to ONNX\n",
      "2022-03-27 23:48:11,676 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:48:11,676 - INFO - Model outputs: ['dense']\n",
      "2022-03-27 23:48:11,676 - INFO - ONNX model is saved at ./torch_model/tf_model_17.onnx\n",
      "2022-03-27 23:48:11,975 - INFO - After optimization: Cast -1 (1->0), Identity -6 (6->0), Transpose -21 (22->1)\n",
      "2022-03-27 23:48:12,478 - INFO - \n",
      "2022-03-27 23:48:12,478 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_18 to ONNX\n",
      "2022-03-27 23:48:12,478 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:48:12,478 - INFO - Model outputs: ['softmax']\n",
      "2022-03-27 23:48:12,478 - INFO - ONNX model is saved at ./torch_model/tf_model_18.onnx\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_2_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_1_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_2_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_2_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_2_BiasAdd_ReadVariableOp_0\"])\n",
      "    dropout_2 = F.relu(StatefulPartitionedCall_model_dense_2_BiasAdd_0)\n",
      "    return dropout_2\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:48:14,490 - INFO - \n",
      "2022-03-27 23:48:14,490 - INFO - Successfully converted TensorFlow model ./torch_model/tf_model_15 to ONNX\n",
      "2022-03-27 23:48:14,490 - INFO - Model inputs: ['conv2d_1_input']\n",
      "2022-03-27 23:48:14,491 - INFO - Model outputs: ['dense_2']\n",
      "2022-03-27 23:48:14,491 - INFO - ONNX model is saved at ./torch_model/tf_model_15.onnx\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_2_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_1_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_2_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_2_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_2_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_2_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_MatMul_ReadVariableOp_0\"])\n",
      "    dense = torch.add(StatefulPartitionedCall_model_dense_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_BiasAdd_ReadVariableOp_0\"])\n",
      "    return dense\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_2_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_1_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_2_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_2_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_2_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_2_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_BiasAdd_ReadVariableOp_0\"])\n",
      "    softmax = F.softmax(StatefulPartitionedCall_model_dense_BiasAdd_0, **{'dim': 1})\n",
      "    return softmax\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for perm of Transpose.\n",
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for dilations of MaxPool.\n",
      "WARNING:root:Cannot get default value for kernel_shape of MaxPool.\n",
      "WARNING:root:Cannot get default value for pads of MaxPool.\n",
      "WARNING:root:Cannot get default value for strides of MaxPool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 96, 'padding': [0, 0], 'kernel_size': (3, 3), 'stride': [2, 2], 'in_channels': 3, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_1_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [2, 2], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 96, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 96, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_2_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 256, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_3_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 384, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_4_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 256, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 384, 'bias': True})\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.weight.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_Conv2D_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd.bias.data = self._vars[\"StatefulPartitionedCall_model_conv2d_5_BiasAdd_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool = nn.MaxPool2d(**{'dilation': 1, 'kernel_size': [3, 3], 'ceil_mode': False, 'stride': [2, 2], 'return_indices': False})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3 = nn.BatchNorm2d(**{'num_features': 256, 'eps': 0.0010000000474974513, 'momentum': 0.8999999761581421})\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.weight.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.bias.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_ReadVariableOp_1_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_mean.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_0\"]\n",
      "    self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3.running_var.data = self._vars[\"StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_ReadVariableOp_1_0\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    conv2d_1_input, = inputs\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0 = conv2d_1_input.permute(*[0, 3, 1, 2])\n",
      "    StatefulPartitionedCall_model_conv2d_1_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_1_BiasAdd(StatefulPartitionedCall_model_conv2d_1_BiasAdd__6_0)\n",
      "    StatefulPartitionedCall_model_conv2d_1_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_1_MaxPool(StatefulPartitionedCall_model_conv2d_1_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_1_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_2_BiasAdd(StatefulPartitionedCall_model_batch_normalization_1_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_2_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_2_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_2_MaxPool(StatefulPartitionedCall_model_conv2d_2_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_2_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_3_BiasAdd(StatefulPartitionedCall_model_batch_normalization_2_FusedBatchNormV3_0)\n",
      "    StatefulPartitionedCall_model_conv2d_3_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_3_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_4_BiasAdd(StatefulPartitionedCall_model_conv2d_3_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_4_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_4_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_BiasAdd_0 = self.n_StatefulPartitionedCall_model_conv2d_5_BiasAdd(StatefulPartitionedCall_model_conv2d_4_Relu_0)\n",
      "    StatefulPartitionedCall_model_conv2d_5_Relu_0 = F.relu(StatefulPartitionedCall_model_conv2d_5_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0 = self.n_StatefulPartitionedCall_model_max_pooling2d_3_MaxPool(StatefulPartitionedCall_model_conv2d_5_Relu_0)\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "    StatefulPartitionedCall_model_flatten_1_Reshape_0 = torch.reshape(StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0, [s if s != 0 else StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0.shape[i] for i, s in enumerate(self._vars[\"const_fold_opt__52\"])])\n",
      "    StatefulPartitionedCall_model_dense_1_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_flatten_1_Reshape_0, self._vars[\"StatefulPartitionedCall_model_dense_1_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_1_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_1_BiasAdd_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_1_Relu_0 = F.relu(StatefulPartitionedCall_model_dense_1_BiasAdd_0)\n",
      "    StatefulPartitionedCall_model_dense_2_MatMul_0 = torch.matmul(StatefulPartitionedCall_model_dense_1_Relu_0, self._vars[\"StatefulPartitionedCall_model_dense_2_MatMul_ReadVariableOp_0\"])\n",
      "    StatefulPartitionedCall_model_dense_2_BiasAdd_0 = torch.add(StatefulPartitionedCall_model_dense_2_MatMul_0, self._vars[\"StatefulPartitionedCall_model_dense_2_BiasAdd_ReadVariableOp_0\"])\n",
      "    dense_2 = F.relu(StatefulPartitionedCall_model_dense_2_BiasAdd_0)\n",
      "    return dense_2\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 32, 32, 3]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_17/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_15/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_16/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py\", line 62, in <module>\n",
      "    predictions = model_torch(x_torch)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"torch_model/layer_18/model.py\", line 76, in forward\n",
      "    StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3_0 = self.n_StatefulPartitionedCall_model_batch_normalization_3_FusedBatchNormV3(StatefulPartitionedCall_model_max_pooling2d_3_MaxPool_0)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2280, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/data/yylaiai/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2248, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yylaiai/fyp21-22/GA/stable/get_prediction_torch.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "o_torch = tf2torch_outputs(model, x, 4)\n",
    "\n",
    "o_torch_np = []\n",
    "for o in o_torch:\n",
    "    o_torch_np.append(o.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4988b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1_input (InputLayer)  [(None, 32, 32, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 96)        2688      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 96)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 96)         384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 256)         614656    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 3, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 3, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 3, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 1, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              1052672   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40970     \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,592,330\n",
      "Trainable params: 21,591,114\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f822bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 15, 15]           2,688\n",
      "         MaxPool2d-2             [-1, 96, 7, 7]               0\n",
      "       BatchNorm2d-3             [-1, 96, 7, 7]             192\n",
      "            Conv2d-4            [-1, 256, 7, 7]         614,656\n",
      "         MaxPool2d-5            [-1, 256, 3, 3]               0\n",
      "       BatchNorm2d-6            [-1, 256, 3, 3]             512\n",
      "            Conv2d-7            [-1, 384, 3, 3]         885,120\n",
      "            Conv2d-8            [-1, 384, 3, 3]       1,327,488\n",
      "            Conv2d-9            [-1, 256, 3, 3]         884,992\n",
      "        MaxPool2d-10            [-1, 256, 1, 1]               0\n",
      "      BatchNorm2d-11            [-1, 256, 1, 1]             512\n",
      "================================================================\n",
      "Total params: 3,716,160\n",
      "Trainable params: 3,716,160\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.44\n",
      "Params size (MB): 14.18\n",
      "Estimated Total Size (MB): 14.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch_model.layer_18.model import Model\n",
    "m = Model()\n",
    "\n",
    "import torchsummary\n",
    "torchsummary.summary(m, (32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd21ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 15, 96]) (1, 16, 16, 96)\n",
      "torch.Size([1, 7, 7, 96]) (1, 8, 8, 96)\n",
      "torch.Size([1, 7, 7, 96]) (1, 8, 8, 96)\n",
      "torch.Size([1, 7, 7, 256]) (1, 8, 8, 256)\n",
      "torch.Size([1, 3, 3, 256]) (1, 3, 3, 256)\n",
      "torch.Size([1, 3, 3, 256]) (1, 3, 3, 256)\n",
      "torch.Size([1, 3, 3, 384]) (1, 3, 3, 384)\n",
      "torch.Size([1, 3, 3, 384]) (1, 3, 3, 384)\n",
      "torch.Size([1, 3, 3, 256]) (1, 3, 3, 256)\n",
      "torch.Size([1, 1, 1, 256]) (1, 1, 1, 256)\n",
      "torch.Size([3, 1, 1, 256]) (1, 1, 1, 256)\n",
      "torch.Size([3, 256]) (1, 256)\n",
      "torch.Size([3, 4096]) (1, 4096)\n",
      "torch.Size([3, 4096]) (1, 4096)\n",
      "torch.Size([3, 4096]) (1, 4096)\n",
      "torch.Size([3, 4096]) (1, 4096)\n",
      "torch.Size([3, 10]) (1, 10)\n",
      "torch.Size([3, 10]) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(o_torch)):\n",
    "    print(o_torch[i].shape, o_tf[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5efe57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,15,15,96) (1,16,16,96) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16969/2419395771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_torch_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \"\"\"\n\u001b[0;32m-> 2189\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fyp_v3/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,15,15,96) (1,16,16,96) "
     ]
    }
   ],
   "source": [
    "for i in range(len(o_tf)):\n",
    "    print(np.allclose(o_torch_np[i], o_tf[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e5677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_v3",
   "language": "python",
   "name": "fyp_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
